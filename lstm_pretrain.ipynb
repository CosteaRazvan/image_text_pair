{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rcostea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rcostea\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "STOP_WORDS_EN = set(stopwords.words('english'))\n",
    "STEMMER = SnowballStemmer('romanian')\n",
    "MAX_LEN = 32\n",
    "WORD_TO_INDEX = None\n",
    "\n",
    "def get_tokens(caption):\n",
    "    caption = caption.lower()\n",
    "    \n",
    "    caption = re.sub(r'[^\\w\\s]', ' ', caption)\n",
    "    caption = re.sub(r'\\s+', ' ', caption)\n",
    "    caption = word_tokenize(caption)\n",
    "    \n",
    "    tokens = []\n",
    "    for word in caption:\n",
    "        if word not in STOP_WORDS_EN:\n",
    "            stemmed_word = STEMMER.stem(word)\n",
    "            tokens.append(stemmed_word)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def build_vocab(tokenized_captions):\n",
    "    vocab = Counter(word for sent in tokenized_captions for word in sent)\n",
    "    \n",
    "    word_to_index = {}\n",
    "    word_to_index[\"<pad>\"] = 0 \n",
    "    word_to_index[\"<unk>\"] = 1\n",
    "\n",
    "    for id, (word, _) in enumerate(vocab.items()):\n",
    "        word_to_index[word] = id+2\n",
    "    \n",
    "    return word_to_index\n",
    "\n",
    "class CaptionsDataset(Dataset):\n",
    "    def __init__(self, root, split):\n",
    "        global WORD_TO_INDEX\n",
    "        csv_file = pd.read_csv(os.path.join(root, f\"{split}.csv\"))\n",
    "        self.split = split\n",
    "\n",
    "        captions = csv_file[\"caption\"].to_numpy()\n",
    "        \n",
    "        tokenized_captions = []\n",
    "        for caption in captions:\n",
    "            tokenized_captions.append(get_tokens(caption))\n",
    "\n",
    "        if split == \"train\":\n",
    "            WORD_TO_INDEX = build_vocab(tokenized_captions)\n",
    "        \n",
    "        encoded_captions = []\n",
    "        for tokenized_caption in tokenized_captions:\n",
    "            encoded_caption = []\n",
    "            for token in tokenized_caption:\n",
    "                if token in WORD_TO_INDEX:\n",
    "                    encoded_caption.append(WORD_TO_INDEX[token])\n",
    "                else:\n",
    "                    encoded_caption.append(WORD_TO_INDEX[\"<unk>\"])\n",
    "            encoded_captions.append(encoded_caption)\n",
    "        \n",
    "        encoded_captions_padded = []\n",
    "        for encoded_caption in encoded_captions:\n",
    "            num_pads = MAX_LEN - len(encoded_caption)\n",
    "            encoded_captions_padded.append(encoded_caption[:MAX_LEN] + [0]*num_pads)\n",
    "        self.data = encoded_captions_padded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample_txt = self.data[idx]\n",
    "\n",
    "        sample_txt = torch.tensor(sample_txt).int()\n",
    "        \n",
    "        return sample_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        if len(params) > 0:\n",
    "            vocab_size = params[\"vocab_size\"]\n",
    "            embed_dim = params[\"embed_dim\"]\n",
    "            hidden_dim = params[\"hidden_dim\"]\n",
    "            num_layers = params[\"num_layers\"]\n",
    "            \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden_state, _) = self.lstm(x)\n",
    "        return hidden_state[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"C:\\Users\\rcostea\\Master\\An2\\DL\\isp-match-dl-2024\"\n",
    "\n",
    "train_dataset = CaptionsDataset(root, \"train\")\n",
    "val_dataset = CaptionsDataset(root, \"val\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "params = {\n",
    "    \"vocab_size\": len(WORD_TO_INDEX), \n",
    "    \"embed_dim\": 256,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_layers\": 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1375,  674,  106, 1731, 1712,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(next(iter(val_dataloader))[0] == 0).nonzero()[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_preds = 0.0\n",
    "    total_preds = 0\n",
    "\n",
    "    for img_inputs, labels in tqdm(train_loader):\n",
    "        img_inputs, labels = img_inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(img_inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct_preds += torch.sum(predictions == labels).item()\n",
    "        total_preds += labels.shape[0]\n",
    "\n",
    "    train_step_loss = total_loss / len(train_loader)\n",
    "    train_step_acc = correct_preds / total_preds\n",
    "\n",
    "    return train_step_loss, train_step_acc\n",
    "\n",
    "def val_step(model, val_loader, optimizer, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_preds = 0.0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_inputs, labels in tqdm(val_loader):\n",
    "            img_inputs, labels = img_inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(img_inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct_preds += torch.sum(predictions == labels).item()\n",
    "            total_preds += labels.shape[0]\n",
    "\n",
    "    val_step_loss = total_loss / len(val_loader)\n",
    "    val_step_acc = correct_preds / total_preds\n",
    "\n",
    "    return val_step_loss, val_step_acc\n",
    "\n",
    "def train():\n",
    "    model = LanguageModel(params).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch: {epoch+1}\")\n",
    "        train_step_loss, train_step_acc = train_step(model, train_dataloader, optimizer, loss_fn)\n",
    "        print(f\"train_step_loss: {train_step_loss} | train_step_acc = {train_step_acc}\")\n",
    "        \n",
    "        val_step_loss, val_step_acc = val_step(model, val_dataloader, optimizer, loss_fn)\n",
    "        print(f\"val_step_loss: {val_step_loss} | val_step_acc = {val_step_acc}\")\n",
    "\n",
    "        train_losses.append(train_step_loss)\n",
    "        train_accs.append(train_step_acc)\n",
    "        val_losses.append(val_step_loss)\n",
    "        val_accs.append(val_step_acc)\n",
    "\n",
    "        if val_step_acc > best_acc:\n",
    "            best_acc = val_step_acc\n",
    "            best_model = model\n",
    "\n",
    "        torch.save(best_model.state_dict(), \"best_vision_model.pt\")\n",
    "\n",
    "    plt.plot(range(epochs), train_losses)\n",
    "    plt.title(\"Train loss\")\n",
    "    plt.savefig(\"train_loss.jpg\")\n",
    "    plt.plot()\n",
    "    \n",
    "    plt.plot(range(epochs), train_accs)\n",
    "    plt.title(\"Train acc\")\n",
    "    plt.savefig(\"train_acc.jpg\")\n",
    "    plt.plot()\n",
    "    \n",
    "    plt.plot(range(epochs), val_losses)\n",
    "    plt.title(\"Val loss\")\n",
    "    plt.savefig(\"val_loss.jpg\")\n",
    "    plt.plot()\n",
    "    \n",
    "    plt.plot(range(epochs), val_accs)\n",
    "    plt.title(\"Val acc\")\n",
    "    plt.savefig(\"val_acc.jpg\")\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 3622, 'embed_dim': 256, 'hidden_dim': 256, 'num_layers': 1}\n",
      "epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff5fe07f867436496e40d4d6fb907fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 67\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m     train_step_loss, train_step_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_step_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_step_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train_step_acc = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_step_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m     val_step_loss, val_step_acc \u001b[38;5;241m=\u001b[39m val_step(model, val_dataloader, optimizer, loss_fn)\n",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, train_loader, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m      4\u001b[0m correct_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      5\u001b[0m total_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m      8\u001b[0m     img_inputs, labels \u001b[38;5;241m=\u001b[39m img_inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
